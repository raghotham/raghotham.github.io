

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Build your own Distribution &mdash; llama-stack  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/css/my_theme.css?v=f1163765" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />

  
    <link rel="canonical" href="https://github.com/meta-llama/llama-stackdistributions/building_distro.html"/>
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=09bf800d"></script>
      <script src="../_static/js/detect_theme.js?v=76226c80"></script>
      <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Building AI Applications (Examples)" href="../building_applications/index.html" />
    <link rel="prev" title="Kubernetes Deployment Guide" href="kubernetes_deployment.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            llama-stack
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Llama Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/index.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/index.html#step-1-install-and-setup">Step 1: Install and setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/index.html#step-2-run-the-llama-stack-server">Step 2: Run the Llama Stack server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/index.html#step-3-run-the-demo">Step 3: Run the demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/index.html#next-steps">Next Steps</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/detailed_tutorial.html">Detailed Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/detailed_tutorial.html#step-1-installation-and-setup">Step 1: Installation and Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/detailed_tutorial.html#step-2-run-llama-stack">Step 2:  Run Llama Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/detailed_tutorial.html#step-3-run-client-cli">Step 3: Run Client CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/detailed_tutorial.html#step-4-run-the-demos">Step 4: Run the Demos</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/index.html">Why Llama Stack?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../introduction/index.html#our-solution-a-universal-stack">Our Solution: A Universal Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../introduction/index.html#our-philosophy">Our Philosophy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../concepts/index.html">Core Concepts</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../concepts/index.html#apis">APIs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../concepts/index.html#api-providers">API Providers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../concepts/index.html#resources">Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="../concepts/index.html#distributions">Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../concepts/index.html#evaluation-concepts">Evaluation Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../concepts/index.html#open-benchmark-eval">Open-benchmark Eval</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../concepts/index.html#list-of-open-benchmarks-llama-stack-support">List of open-benchmarks Llama Stack support</a></li>
<li class="toctree-l4"><a class="reference internal" href="../concepts/index.html#run-evaluation-on-open-benchmarks-via-cli">Run evaluation on open-benchmarks via CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../concepts/index.html#spin-up-llama-stack-server">Spin up Llama Stack server</a></li>
<li class="toctree-l4"><a class="reference internal" href="../concepts/index.html#run-eval-cli">Run eval CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../concepts/index.html#whats-next">What’s Next?</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../openai/index.html">OpenAI API Compatibility</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../openai/index.html#server-path">Server path</a></li>
<li class="toctree-l2"><a class="reference internal" href="../openai/index.html#clients">Clients</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../openai/index.html#llama-stack-client">Llama Stack Client</a></li>
<li class="toctree-l3"><a class="reference internal" href="../openai/index.html#openai-client">OpenAI Client</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../openai/index.html#apis-implemented">APIs implemented</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../openai/index.html#models">Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../openai/index.html#responses">Responses</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../openai/index.html#simple-inference">Simple inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../openai/index.html#structured-output">Structured Output</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../openai/index.html#chat-completions">Chat Completions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../openai/index.html#id1">Simple inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../openai/index.html#id2">Structured Output</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../openai/index.html#completions">Completions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../openai/index.html#id3">Simple inference</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../providers/index.html">Providers Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../providers/index.html#external-providers">External Providers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../providers/index.html#agents">Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../providers/index.html#datasetio">DatasetIO</a></li>
<li class="toctree-l2"><a class="reference internal" href="../providers/index.html#eval">Eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../providers/index.html#inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../providers/index.html#post-training">Post Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../providers/index.html#post-training-providers">Post Training Providers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../providers/external.html">External Providers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../providers/post_training/huggingface.html">HuggingFace SFTTrainer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../providers/post_training/torchtune.html">TorchTune</a></li>
<li class="toctree-l4"><a class="reference internal" href="../providers/post_training/nvidia_nemo.html">NVIDIA NEMO</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../providers/index.html#safety">Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../providers/index.html#scoring">Scoring</a></li>
<li class="toctree-l2"><a class="reference internal" href="../providers/index.html#telemetry">Telemetry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../providers/index.html#tool-runtime">Tool Runtime</a></li>
<li class="toctree-l2"><a class="reference internal" href="../providers/index.html#vector-io">Vector IO</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../providers/index.html#vector-io-providers">Vector IO Providers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../providers/external.html">External Providers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../providers/vector_io/faiss.html">Faiss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../providers/vector_io/sqlite-vec.html">SQLite-Vec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../providers/vector_io/chromadb.html">Chroma</a></li>
<li class="toctree-l4"><a class="reference internal" href="../providers/vector_io/pgvector.html">Postgres PGVector</a></li>
<li class="toctree-l4"><a class="reference internal" href="../providers/vector_io/qdrant.html">Qdrant</a></li>
<li class="toctree-l4"><a class="reference internal" href="../providers/vector_io/milvus.html">Milvus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../providers/vector_io/weaviate.html">Weaviate</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Distributions Overview</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="importing_as_library.html">Using Llama Stack as a Library</a><ul>
<li class="toctree-l3"><a class="reference internal" href="importing_as_library.html#setup-llama-stack-without-a-server">Setup Llama Stack without a Server</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html">Configuring a “Stack”</a><ul>
<li class="toctree-l3"><a class="reference internal" href="configuration.html#providers">Providers</a></li>
<li class="toctree-l3"><a class="reference internal" href="configuration.html#resources">Resources</a></li>
<li class="toctree-l3"><a class="reference internal" href="configuration.html#server-configuration">Server Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="configuration.html#authentication-configuration">Authentication Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="configuration.html#quota-configuration">Quota Configuration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="configuration.html#extending-to-handle-safety">Extending to handle Safety</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="list_of_distributions.html">Available List of Distributions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="list_of_distributions.html#selection-of-a-distribution-template">Selection of a Distribution / Template</a><ul>
<li class="toctree-l4"><a class="reference internal" href="list_of_distributions.html#distribution-details">Distribution Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="list_of_distributions.html#on-device-distributions">On-Device Distributions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="kubernetes_deployment.html">Kubernetes Deployment Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="kubernetes_deployment.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="kubernetes_deployment.html#deploying-llama-stack-server-in-kubernetes">Deploying Llama Stack Server in Kubernetes</a></li>
<li class="toctree-l3"><a class="reference internal" href="kubernetes_deployment.html#verifying-the-deployment">Verifying the Deployment</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Build your own Distribution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#setting-your-log-level">Setting your log level</a></li>
<li class="toctree-l3"><a class="reference internal" href="#llama-stack-build">Llama Stack Build</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-your-stack-server">Running your Stack server</a></li>
<li class="toctree-l3"><a class="reference internal" href="#listing-distributions">Listing Distributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#removing-a-distribution">Removing a Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#troubleshooting">Troubleshooting</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../building_applications/index.html">Building AI Applications (Examples)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../building_applications/rag.html">Retrieval Augmented Generation (RAG)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/rag.html#setting-up-vector-dbs">Setting up Vector DBs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/rag.html#ingesting-documents">Ingesting Documents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../building_applications/rag.html#using-precomputed-embeddings">Using Precomputed Embeddings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/rag.html#retrieval">Retrieval</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/rag.html#using-the-rag-tool">Using the RAG Tool</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/rag.html#building-rag-enhanced-agents">Building RAG-Enhanced Agents</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/rag.html#unregistering-vector-dbs">Unregistering Vector DBs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/rag.html#appendix">Appendix</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../building_applications/rag.html#more-ragdocument-examples">More RAGDocument Examples</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../building_applications/agent.html">Agents</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/agent.html#core-concepts">Core Concepts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../building_applications/agent.html#agent-configuration">1. Agent Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../building_applications/agent.html#sessions">2. Sessions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../building_applications/agent.html#turns">3. Turns</a></li>
<li class="toctree-l4"><a class="reference internal" href="../building_applications/agent.html#non-streaming">Non-Streaming</a></li>
<li class="toctree-l4"><a class="reference internal" href="../building_applications/agent.html#steps">4. Steps</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/agent.html#agent-execution-loop">Agent Execution Loop</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../building_applications/agent_execution_loop.html">Agent Execution Loop</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/agent_execution_loop.html#steps-in-the-agent-workflow">Steps in the Agent Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/agent_execution_loop.html#agent-execution-loop-example">Agent Execution Loop Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../building_applications/tools.html">Tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/tools.html#server-side-vs-client-side-tool-execution">Server-side vs. client-side tool execution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../building_applications/tools.html#server-side-tools">Server-side tools</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/tools.html#model-context-protocol-mcp">Model Context Protocol (MCP)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../building_applications/tools.html#using-remote-mcp-servers">Using Remote MCP Servers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../building_applications/tools.html#running-your-own-mcp-server">Running your own MCP server</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/tools.html#adding-custom-client-side-tools">Adding Custom (Client-side) Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/tools.html#tool-invocation">Tool Invocation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/tools.html#listing-available-tools">Listing Available Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/tools.html#simple-example-2-using-an-agent-with-the-web-search-tool">Simple Example 2: Using an Agent with the Web Search Tool</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/tools.html#simple-example3-using-an-agent-with-the-wolframalpha-tool">Simple Example3: Using an Agent with the WolframAlpha Tool</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../building_applications/evals.html">Evaluations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/evals.html#application-evaluation">Application Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../building_applications/evals.html#building-a-search-agent">Building a Search Agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="../building_applications/evals.html#query-agent-execution-steps">Query Agent Execution Steps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../building_applications/evals.html#evaluate-agent-responses">Evaluate Agent Responses</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../building_applications/telemetry.html">Telemetry</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/telemetry.html#events">Events</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/telemetry.html#spans-and-traces">Spans and Traces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/telemetry.html#sinks">Sinks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/telemetry.html#providers">Providers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../building_applications/telemetry.html#meta-reference-provider">Meta-Reference Provider</a></li>
<li class="toctree-l4"><a class="reference internal" href="../building_applications/telemetry.html#configuration">Configuration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/telemetry.html#jaeger-to-visualize-traces">Jaeger to visualize traces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_applications/telemetry.html#querying-traces-stored-in-sqlite">Querying Traces Stored in SQLite</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../building_applications/safety.html">Safety Guardrails</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../playground/index.html">Llama Stack Playground</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../playground/index.html#key-features">Key Features</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../playground/index.html#playground">Playground</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../playground/index.html#chatbot">Chatbot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../playground/index.html#evaluations">Evaluations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../playground/index.html#inspect">Inspect</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../playground/index.html#starting-the-llama-stack-playground">Starting the Llama Stack Playground</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../contributing/new_api_provider.html">Adding a New API Provider</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../contributing/new_api_provider.html#testing-the-provider">Testing the Provider</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing/new_api_provider.html#integration-testing">1. Integration Testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing/new_api_provider.html#unit-testing">2. Unit Testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing/new_api_provider.html#additional-end-to-end-testing">3. Additional end-to-end testing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../contributing/new_api_provider.html#submitting-your-pr">Submitting Your PR</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../references/index.html">References</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../references/api_reference/index.html">API Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../references/python_sdk_reference/index.html">Python SDK Reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#shared-types">Shared Types</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#toolgroups">Toolgroups</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#tools">Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#toolruntime">ToolRuntime</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/python_sdk_reference/index.html#ragtool">RagTool</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#agents">Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/python_sdk_reference/index.html#session">Session</a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/python_sdk_reference/index.html#steps">Steps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/python_sdk_reference/index.html#turn">Turn</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#batchinference">BatchInference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#datasets">Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#eval">Eval</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/python_sdk_reference/index.html#jobs">Jobs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#inspect">Inspect</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#inference">Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#vectorio">VectorIo</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#vectordbs">VectorDBs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#models">Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#posttraining">PostTraining</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/python_sdk_reference/index.html#job">Job</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#providers">Providers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#routes">Routes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#safety">Safety</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#shields">Shields</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#syntheticdatageneration">SyntheticDataGeneration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#telemetry">Telemetry</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#datasetio">Datasetio</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#scoring">Scoring</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#scoringfunctions">ScoringFunctions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/python_sdk_reference/index.html#benchmarks">Benchmarks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../references/llama_cli_reference/index.html">llama (server-side) CLI Reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../references/llama_cli_reference/index.html#installation">Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/llama_cli_reference/index.html#llama-subcommands"><code class="docutils literal notranslate"><span class="pre">llama</span></code> subcommands</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_cli_reference/index.html#sample-usage">Sample Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../references/llama_cli_reference/index.html#downloading-models">Downloading models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_cli_reference/index.html#downloading-from-meta">Downloading from Meta</a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_cli_reference/index.html#downloading-from-hugging-face">Downloading from Hugging Face</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../references/llama_cli_reference/index.html#list-the-downloaded-models">List the downloaded models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/llama_cli_reference/index.html#understand-the-models">Understand the models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_cli_reference/index.html#id1">Sample Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_cli_reference/index.html#describe">Describe</a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_cli_reference/index.html#prompt-format">Prompt Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_cli_reference/index.html#remove-model">Remove model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html">llama (client-side) CLI Reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#basic-commands">Basic Commands</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-configure"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">configure</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-providers-list"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">providers</span> <span class="pre">list</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#model-management">Model Management</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-models-list"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">models</span> <span class="pre">list</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-models-get"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">models</span> <span class="pre">get</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-models-register"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">models</span> <span class="pre">register</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-models-update"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">models</span> <span class="pre">update</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-models-delete"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">models</span> <span class="pre">delete</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#vector-db-management">Vector DB Management</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-vector-dbs-list"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">vector_dbs</span> <span class="pre">list</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-vector-dbs-register"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">vector_dbs</span> <span class="pre">register</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-vector-dbs-unregister"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">vector_dbs</span> <span class="pre">unregister</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#shield-management">Shield Management</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-shields-list"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">shields</span> <span class="pre">list</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-shields-register"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">shields</span> <span class="pre">register</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#eval-task-management">Eval Task Management</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-benchmarks-list"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">benchmarks</span> <span class="pre">list</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-benchmarks-register"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">benchmarks</span> <span class="pre">register</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#eval-execution">Eval execution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-eval-run-benchmark"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">eval</span> <span class="pre">run-benchmark</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-eval-run-scoring"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">eval</span> <span class="pre">run-scoring</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#tool-group-management">Tool Group Management</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-toolgroups-list"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">toolgroups</span> <span class="pre">list</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-toolgroups-get"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">toolgroups</span> <span class="pre">get</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-toolgroups-register"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">toolgroups</span> <span class="pre">register</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_stack_client_cli_reference.html#llama-stack-client-toolgroups-unregister"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">toolgroups</span> <span class="pre">unregister</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../references/llama_cli_reference/download_models.html">Downloading Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../references/llama_cli_reference/download_models.html#installation">Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/llama_cli_reference/download_models.html#downloading-models-via-cli">Downloading models via CLI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_cli_reference/download_models.html#downloading-from-meta">Downloading from Meta</a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/llama_cli_reference/download_models.html#downloading-from-hugging-face">Downloading from Hugging Face</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../references/llama_cli_reference/download_models.html#list-the-downloaded-models">List the downloaded models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../references/evals_reference/index.html">Evaluations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../references/evals_reference/index.html#evaluation-concepts">Evaluation Concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../references/evals_reference/index.html#evaluation-examples-walkthrough">Evaluation Examples Walkthrough</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/evals_reference/index.html#open-benchmark-model-evaluation">1. Open Benchmark Model Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/evals_reference/index.html#agentic-evaluation">2. Agentic Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/evals_reference/index.html#agentic-application-dataset-scoring">3. Agentic Application Dataset Scoring</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../references/evals_reference/index.html#running-evaluations-via-cli">Running Evaluations via CLI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/evals_reference/index.html#benchmark-evaluation-cli">Benchmark Evaluation CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/evals_reference/index.html#application-evaluation-cli">Application Evaluation CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/evals_reference/index.html#defining-benchmarkconfig">Defining BenchmarkConfig</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../references/evals_reference/index.html#open-benchmark-contributing-guide">Open-benchmark Contributing Guide</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../references/evals_reference/index.html#create-the-new-dataset-for-your-new-benchmark">Create the new dataset for your new benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/evals_reference/index.html#find-scoring-function-for-your-new-benchmark">Find scoring function for your new benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/evals_reference/index.html#add-new-benchmark-into-template">Add new benchmark into template</a></li>
<li class="toctree-l4"><a class="reference internal" href="../references/evals_reference/index.html#test-the-new-benchmark">Test the new benchmark</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">llama-stack</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Distributions Overview</a></li>
      <li class="breadcrumb-item active">Build your own Distribution</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/distributions/building_distro.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="build-your-own-distribution">
<h1>Build your own Distribution<a class="headerlink" href="#build-your-own-distribution" title="Link to this heading"></a></h1>
<p>This guide will walk you through the steps to get started with building a Llama Stack distribution from scratch with your choice of API providers.</p>
<section id="setting-your-log-level">
<h2>Setting your log level<a class="headerlink" href="#setting-your-log-level" title="Link to this heading"></a></h2>
<p>In order to specify the proper logging level users can apply the following environment variable <code class="docutils literal notranslate"><span class="pre">LLAMA_STACK_LOGGING</span></code> with the following format:</p>
<p><code class="docutils literal notranslate"><span class="pre">LLAMA_STACK_LOGGING=server=debug;core=info</span></code></p>
<p>Where each category in the following list:</p>
<ul class="simple">
<li><p>all</p></li>
<li><p>core</p></li>
<li><p>server</p></li>
<li><p>router</p></li>
<li><p>inference</p></li>
<li><p>agents</p></li>
<li><p>safety</p></li>
<li><p>eval</p></li>
<li><p>tools</p></li>
<li><p>client</p></li>
</ul>
<p>Can be set to any of the following log levels:</p>
<ul class="simple">
<li><p>debug</p></li>
<li><p>info</p></li>
<li><p>warning</p></li>
<li><p>error</p></li>
<li><p>critical</p></li>
</ul>
<p>The default global log level is <code class="docutils literal notranslate"><span class="pre">info</span></code>. <code class="docutils literal notranslate"><span class="pre">all</span></code> sets the log level for all components.</p>
<p>A user can also set <code class="docutils literal notranslate"><span class="pre">LLAMA_STACK_LOG_FILE</span></code> which will pipe the logs to the specified path as well as to the terminal. An example would be: <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">LLAMA_STACK_LOG_FILE=server.log</span></code></p>
</section>
<section id="llama-stack-build">
<h2>Llama Stack Build<a class="headerlink" href="#llama-stack-build" title="Link to this heading"></a></h2>
<p>In order to build your own distribution, we recommend you clone the <code class="docutils literal notranslate"><span class="pre">llama-stack</span></code> repository.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">git</span><span class="nd">@github</span><span class="o">.</span><span class="n">com</span><span class="p">:</span><span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">llama</span><span class="o">-</span><span class="n">stack</span><span class="o">.</span><span class="n">git</span>
<span class="n">cd</span> <span class="n">llama</span><span class="o">-</span><span class="n">stack</span>
<span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">e</span> <span class="o">.</span>
</pre></div>
</div>
<p>Use the CLI to build your distribution.
The main points to consider are:</p>
<ol class="arabic simple">
<li><p><strong>Image Type</strong> - Do you want a Conda / venv environment or a Container (eg. Docker)</p></li>
<li><p><strong>Template</strong> - Do you want to use a template to build your distribution? or start from scratch ?</p></li>
<li><p><strong>Config</strong> - Do you want to use a pre-existing config file to build your distribution?</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>llama stack build -h
usage: llama stack build [-h] [--config CONFIG] [--template TEMPLATE] [--list-templates] [--image-type {conda,container,venv}] [--image-name IMAGE_NAME] [--print-deps-only] [--run]

Build a Llama stack container

options:
  -h, --help            show this help message and exit
  --config CONFIG       Path to a config file to use for the build. You can find example configs in llama_stack/distributions/**/build.yaml. If this argument is not provided, you will
                        be prompted to enter information interactively (default: None)
  --template TEMPLATE   Name of the example template config to use for build. You may use `llama stack build --list-templates` to check out the available templates (default: None)
  --list-templates      Show the available templates for building a Llama Stack distribution (default: False)
  --image-type {conda,container,venv}
                        Image Type to use for the build. This can be either conda or container or venv. If not specified, will use the image type from the template config. (default:
                        conda)
  --image-name IMAGE_NAME
                        [for image-type=conda|container|venv] Name of the conda or virtual environment to use for the build. If not specified, currently active Conda environment will be used if
                        found. (default: None)
  --print-deps-only     Print the dependencies for the stack only, without building the stack (default: False)
  --run                 Run the stack after building using the same image type, name, and other applicable arguments (default: False)

</pre></div>
</div>
<p>After this step is complete, a file named <code class="docutils literal notranslate"><span class="pre">&lt;name&gt;-build.yaml</span></code> and template file <code class="docutils literal notranslate"><span class="pre">&lt;name&gt;-run.yaml</span></code> will be generated and saved at the output file path specified at the end of the command.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
Building from a template</label><div class="sd-tab-content docutils">
<p>To build from alternative API providers, we provide distribution templates for users to get started building a distribution backed by different providers.</p>
<p>The following command will allow you to see the available templates and their corresponding providers.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="n">build</span> <span class="o">--</span><span class="nb">list</span><span class="o">-</span><span class="n">templates</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">Template</span> <span class="n">Name</span>                <span class="o">|</span> <span class="n">Description</span>                                                                 <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">hf</span><span class="o">-</span><span class="n">serverless</span>                <span class="o">|</span> <span class="n">Use</span> <span class="p">(</span><span class="n">an</span> <span class="n">external</span><span class="p">)</span> <span class="n">Hugging</span> <span class="n">Face</span> <span class="n">Inference</span> <span class="n">Endpoint</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span> <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">together</span>                     <span class="o">|</span> <span class="n">Use</span> <span class="n">Together</span><span class="o">.</span><span class="n">AI</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                                   <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">vllm</span><span class="o">-</span><span class="n">gpu</span>                     <span class="o">|</span> <span class="n">Use</span> <span class="n">a</span> <span class="n">built</span><span class="o">-</span><span class="ow">in</span> <span class="n">vLLM</span> <span class="n">engine</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                        <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">experimental</span><span class="o">-</span><span class="n">post</span><span class="o">-</span><span class="n">training</span>   <span class="o">|</span> <span class="n">Experimental</span> <span class="n">template</span> <span class="k">for</span> <span class="n">post</span> <span class="n">training</span>                                     <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">remote</span><span class="o">-</span><span class="n">vllm</span>                  <span class="o">|</span> <span class="n">Use</span> <span class="p">(</span><span class="n">an</span> <span class="n">external</span><span class="p">)</span> <span class="n">vLLM</span> <span class="n">server</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                     <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">fireworks</span>                    <span class="o">|</span> <span class="n">Use</span> <span class="n">Fireworks</span><span class="o">.</span><span class="n">AI</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                                  <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">tgi</span>                          <span class="o">|</span> <span class="n">Use</span> <span class="p">(</span><span class="n">an</span> <span class="n">external</span><span class="p">)</span> <span class="n">TGI</span> <span class="n">server</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                      <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">bedrock</span>                      <span class="o">|</span> <span class="n">Use</span> <span class="n">AWS</span> <span class="n">Bedrock</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span> <span class="ow">and</span> <span class="n">safety</span>                        <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">meta</span><span class="o">-</span><span class="n">reference</span><span class="o">-</span><span class="n">gpu</span>           <span class="o">|</span> <span class="n">Use</span> <span class="n">Meta</span> <span class="n">Reference</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                                <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">nvidia</span>                       <span class="o">|</span> <span class="n">Use</span> <span class="n">NVIDIA</span> <span class="n">NIM</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                                    <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">cerebras</span>                     <span class="o">|</span> <span class="n">Use</span> <span class="n">Cerebras</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                                      <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">ollama</span>                       <span class="o">|</span> <span class="n">Use</span> <span class="p">(</span><span class="n">an</span> <span class="n">external</span><span class="p">)</span> <span class="n">Ollama</span> <span class="n">server</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                   <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">hf</span><span class="o">-</span><span class="n">endpoint</span>                  <span class="o">|</span> <span class="n">Use</span> <span class="p">(</span><span class="n">an</span> <span class="n">external</span><span class="p">)</span> <span class="n">Hugging</span> <span class="n">Face</span> <span class="n">Inference</span> <span class="n">Endpoint</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span> <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
</pre></div>
</div>
<p>You may then pick a template to build your distribution with providers fitted to your liking.</p>
<p>For example, to build a distribution with TGI as the inference provider, you can run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ llama stack build --template tgi
...
You can now edit ~/.llama/distributions/llamastack-tgi/tgi-run.yaml and run `llama stack run ~/.llama/distributions/llamastack-tgi/tgi-run.yaml`
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
Building from Scratch</label><div class="sd-tab-content docutils">
<p>If the provided templates do not fit your use case, you could start off with running <code class="docutils literal notranslate"><span class="pre">llama</span> <span class="pre">stack</span> <span class="pre">build</span></code> which will allow you to a interactively enter wizard where you will be prompted to enter build configurations.</p>
<p>It would be best to start with a template and understand the structure of the config file and the various concepts ( APIS, providers, resources, etc.) before starting from scratch.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>llama stack build

&gt; Enter a name for your Llama Stack (e.g. my-local-stack): my-stack
&gt; Enter the image type you want your Llama Stack to be built as (container or conda or venv): conda

Llama Stack is composed of several APIs working together. Let&#39;s select
the provider types (implementations) you want to use for these APIs.

Tip: use &lt;TAB&gt; to see options for the providers.

&gt; Enter provider for API inference: inline::meta-reference
&gt; Enter provider for API safety: inline::llama-guard
&gt; Enter provider for API agents: inline::meta-reference
&gt; Enter provider for API memory: inline::faiss
&gt; Enter provider for API datasetio: inline::meta-reference
&gt; Enter provider for API scoring: inline::meta-reference
&gt; Enter provider for API eval: inline::meta-reference
&gt; Enter provider for API telemetry: inline::meta-reference

 &gt; (Optional) Enter a short description for your Llama Stack:

You can now edit ~/.llama/distributions/llamastack-my-local-stack/my-local-stack-run.yaml and run `llama stack run ~/.llama/distributions/llamastack-my-local-stack/my-local-stack-run.yaml`
</pre></div>
</div>
</div>
<input id="sd-tab-item-2" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-2">
Building from a pre-existing build config file</label><div class="sd-tab-content docutils">
<ul class="simple">
<li><p>In addition to templates, you may customize the build to your liking through editing config files and build from config files with the following command.</p></li>
<li><p>The config file will be of contents like the ones in <code class="docutils literal notranslate"><span class="pre">llama_stack/templates/*build.yaml</span></code>.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ cat llama_stack/templates/ollama/build.yaml

name: ollama
distribution_spec:
  description: Like local, but use ollama for running LLM inference
  providers:
    inference: remote::ollama
    memory: inline::faiss
    safety: inline::llama-guard
    agents: inline::meta-reference
    telemetry: inline::meta-reference
image_name: ollama
image_type: conda

# If some providers are external, you can specify the path to the implementation
external_providers_dir: ~/.llama/providers.d
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="n">build</span> <span class="o">--</span><span class="n">config</span> <span class="n">llama_stack</span><span class="o">/</span><span class="n">templates</span><span class="o">/</span><span class="n">ollama</span><span class="o">/</span><span class="n">build</span><span class="o">.</span><span class="n">yaml</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-3" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-3">
Building with External Providers</label><div class="sd-tab-content docutils">
<p>Llama Stack supports external providers that live outside of the main codebase. This allows you to create and maintain your own providers independently or use community-provided providers.</p>
<p>To build a distribution with external providers, you need to:</p>
<ol class="arabic simple">
<li><p>Configure the <code class="docutils literal notranslate"><span class="pre">external_providers_dir</span></code> in your build configuration file:</p></li>
</ol>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example my-external-stack.yaml with external providers</span>
<span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;2&#39;</span>
<span class="nt">distribution_spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Custom distro for CI tests</span>
<span class="w">  </span><span class="nt">providers</span><span class="p">:</span>
<span class="w">    </span><span class="nt">inference</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">remote::custom_ollama</span>
<span class="c1"># Add more providers as needed</span>
<span class="nt">image_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">container</span>
<span class="nt">image_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ci-test</span>
<span class="c1"># Path to external provider implementations</span>
<span class="nt">external_providers_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">~/.llama/providers.d</span>
</pre></div>
</div>
<p>Here’s an example for a custom Ollama provider:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">adapter</span><span class="p">:</span>
<span class="w">  </span><span class="nt">adapter_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">custom_ollama</span>
<span class="w">  </span><span class="nt">pip_packages</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aiohttp</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llama-stack-provider-ollama</span><span class="w"> </span><span class="c1"># This is the provider package</span>
<span class="w">  </span><span class="nt">config_class</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llama_stack_ollama_provider.config.OllamaImplConfig</span>
<span class="w">  </span><span class="nt">module</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llama_stack_ollama_provider</span>
<span class="nt">api_dependencies</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>
<span class="nt">optional_api_dependencies</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">pip_packages</span></code> section lists the Python packages required by the provider, as well as the
provider package itself. The package must be available on PyPI or can be provided from a local
directory or a git repository (git must be installed on the build environment).</p>
<ol class="arabic simple" start="2">
<li><p>Build your distribution using the config file:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="n">build</span> <span class="o">--</span><span class="n">config</span> <span class="n">my</span><span class="o">-</span><span class="n">external</span><span class="o">-</span><span class="n">stack</span><span class="o">.</span><span class="n">yaml</span>
</pre></div>
</div>
<p>For more information on external providers, including directory structure, provider types, and implementation requirements, see the <a class="reference internal" href="../providers/external.html"><span class="std std-doc">External Providers documentation</span></a>.</p>
</div>
<input id="sd-tab-item-4" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-4">
Building Container</label><div class="sd-tab-content docutils">
<div class="tip admonition">
<p class="admonition-title">Podman Alternative</p>
<p>Podman is supported as an alternative to Docker. Set <code class="docutils literal notranslate"><span class="pre">CONTAINER_BINARY</span></code> to <code class="docutils literal notranslate"><span class="pre">podman</span></code> in your environment to use Podman.</p>
</div>
<p>To build a container image, you may start off from a template and use the <code class="docutils literal notranslate"><span class="pre">--image-type</span> <span class="pre">container</span></code> flag to specify <code class="docutils literal notranslate"><span class="pre">container</span></code> as the build image type.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="n">build</span> <span class="o">--</span><span class="n">template</span> <span class="n">ollama</span> <span class="o">--</span><span class="n">image</span><span class="o">-</span><span class="nb">type</span> <span class="n">container</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ llama stack build --template ollama --image-type container
...
Containerfile created successfully in /tmp/tmp.viA3a3Rdsg/ContainerfileFROM python:3.10-slim
...

You can now edit ~/meta-llama/llama-stack/tmp/configs/ollama-run.yaml and run `llama stack run ~/meta-llama/llama-stack/tmp/configs/ollama-run.yaml`
</pre></div>
</div>
<p>Now set some environment variables for the inference model ID and Llama Stack Port and create a local directory to mount into the container’s file system.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">INFERENCE_MODEL</span><span class="o">=</span><span class="s2">&quot;llama3.2:3b&quot;</span>
<span class="n">export</span> <span class="n">LLAMA_STACK_PORT</span><span class="o">=</span><span class="mi">8321</span>
<span class="n">mkdir</span> <span class="o">-</span><span class="n">p</span> <span class="o">~/.</span><span class="n">llama</span>
</pre></div>
</div>
<p>After this step is successful, you should be able to find the built container image and test it with the below Docker command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>docker run -d \
  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \
  -v ~/.llama:/root/.llama \
  localhost/distribution-ollama:dev \
  --port $LLAMA_STACK_PORT \
  --env INFERENCE_MODEL=$INFERENCE_MODEL \
  --env OLLAMA_URL=http://host.docker.internal:11434
</pre></div>
</div>
<p>Here are the docker flags and their uses:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-d</span></code>: Runs the container in the detached mode as a background process</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-p</span> <span class="pre">$LLAMA_STACK_PORT:$LLAMA_STACK_PORT</span></code>: Maps the container port to the host port for accessing the server</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-v</span> <span class="pre">~/.llama:/root/.llama</span></code>: Mounts the local .llama directory to persist configurations and data</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">localhost/distribution-ollama:dev</span></code>: The name and tag of the container image to run</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--port</span> <span class="pre">$LLAMA_STACK_PORT</span></code>: Port number for the server to listen on</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--env</span> <span class="pre">INFERENCE_MODEL=$INFERENCE_MODEL</span></code>: Sets the model to use for inference</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--env</span> <span class="pre">OLLAMA_URL=http://host.docker.internal:11434</span></code>: Configures the URL for the Ollama service</p></li>
</ul>
</div>
</div>
</section>
<section id="running-your-stack-server">
<h2>Running your Stack server<a class="headerlink" href="#running-your-stack-server" title="Link to this heading"></a></h2>
<p>Now, let’s start the Llama Stack Distribution Server. You will need the YAML configuration file which was written out at the end by the <code class="docutils literal notranslate"><span class="pre">llama</span> <span class="pre">stack</span> <span class="pre">build</span></code> step.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="n">run</span> <span class="o">-</span><span class="n">h</span>
<span class="n">usage</span><span class="p">:</span> <span class="n">llama</span> <span class="n">stack</span> <span class="n">run</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">port</span> <span class="n">PORT</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">image</span><span class="o">-</span><span class="n">name</span> <span class="n">IMAGE_NAME</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">env</span> <span class="n">KEY</span><span class="o">=</span><span class="n">VALUE</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">tls</span><span class="o">-</span><span class="n">keyfile</span> <span class="n">TLS_KEYFILE</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">tls</span><span class="o">-</span><span class="n">certfile</span> <span class="n">TLS_CERTFILE</span><span class="p">]</span>
                       <span class="p">[</span><span class="o">--</span><span class="n">image</span><span class="o">-</span><span class="nb">type</span> <span class="p">{</span><span class="n">conda</span><span class="p">,</span><span class="n">container</span><span class="p">,</span><span class="n">venv</span><span class="p">}]</span>
                       <span class="n">config</span>

<span class="n">Start</span> <span class="n">the</span> <span class="n">server</span> <span class="k">for</span> <span class="n">a</span> <span class="n">Llama</span> <span class="n">Stack</span> <span class="n">Distribution</span><span class="o">.</span> <span class="n">You</span> <span class="n">should</span> <span class="n">have</span> <span class="n">already</span> <span class="n">built</span> <span class="p">(</span><span class="ow">or</span> <span class="n">downloaded</span><span class="p">)</span> <span class="ow">and</span> <span class="n">configured</span> <span class="n">the</span> <span class="n">distribution</span><span class="o">.</span>

<span class="n">positional</span> <span class="n">arguments</span><span class="p">:</span>
  <span class="n">config</span>                <span class="n">Path</span> <span class="n">to</span> <span class="n">config</span> <span class="n">file</span> <span class="n">to</span> <span class="n">use</span> <span class="k">for</span> <span class="n">the</span> <span class="n">run</span>

<span class="n">options</span><span class="p">:</span>
  <span class="o">-</span><span class="n">h</span><span class="p">,</span> <span class="o">--</span><span class="n">help</span>            <span class="n">show</span> <span class="n">this</span> <span class="n">help</span> <span class="n">message</span> <span class="ow">and</span> <span class="n">exit</span>
  <span class="o">--</span><span class="n">port</span> <span class="n">PORT</span>           <span class="n">Port</span> <span class="n">to</span> <span class="n">run</span> <span class="n">the</span> <span class="n">server</span> <span class="n">on</span><span class="o">.</span> <span class="n">It</span> <span class="n">can</span> <span class="n">also</span> <span class="n">be</span> <span class="n">passed</span> <span class="n">via</span> <span class="n">the</span> <span class="n">env</span> <span class="n">var</span> <span class="n">LLAMA_STACK_PORT</span><span class="o">.</span> <span class="p">(</span><span class="n">default</span><span class="p">:</span> <span class="mi">8321</span><span class="p">)</span>
  <span class="o">--</span><span class="n">image</span><span class="o">-</span><span class="n">name</span> <span class="n">IMAGE_NAME</span>
                        <span class="n">Name</span> <span class="n">of</span> <span class="n">the</span> <span class="n">image</span> <span class="n">to</span> <span class="n">run</span><span class="o">.</span> <span class="n">Defaults</span> <span class="n">to</span> <span class="n">the</span> <span class="n">current</span> <span class="n">environment</span> <span class="p">(</span><span class="n">default</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
  <span class="o">--</span><span class="n">env</span> <span class="n">KEY</span><span class="o">=</span><span class="n">VALUE</span>       <span class="n">Environment</span> <span class="n">variables</span> <span class="n">to</span> <span class="k">pass</span> <span class="n">to</span> <span class="n">the</span> <span class="n">server</span> <span class="ow">in</span> <span class="n">KEY</span><span class="o">=</span><span class="n">VALUE</span> <span class="nb">format</span><span class="o">.</span> <span class="n">Can</span> <span class="n">be</span> <span class="n">specified</span> <span class="n">multiple</span> <span class="n">times</span><span class="o">.</span> <span class="p">(</span><span class="n">default</span><span class="p">:</span> <span class="p">[])</span>
  <span class="o">--</span><span class="n">tls</span><span class="o">-</span><span class="n">keyfile</span> <span class="n">TLS_KEYFILE</span>
                        <span class="n">Path</span> <span class="n">to</span> <span class="n">TLS</span> <span class="n">key</span> <span class="n">file</span> <span class="k">for</span> <span class="n">HTTPS</span> <span class="p">(</span><span class="n">default</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
  <span class="o">--</span><span class="n">tls</span><span class="o">-</span><span class="n">certfile</span> <span class="n">TLS_CERTFILE</span>
                        <span class="n">Path</span> <span class="n">to</span> <span class="n">TLS</span> <span class="n">certificate</span> <span class="n">file</span> <span class="k">for</span> <span class="n">HTTPS</span> <span class="p">(</span><span class="n">default</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
  <span class="o">--</span><span class="n">image</span><span class="o">-</span><span class="nb">type</span> <span class="p">{</span><span class="n">conda</span><span class="p">,</span><span class="n">container</span><span class="p">,</span><span class="n">venv</span><span class="p">}</span>
                        <span class="n">Image</span> <span class="n">Type</span> <span class="n">used</span> <span class="n">during</span> <span class="n">the</span> <span class="n">build</span><span class="o">.</span> <span class="n">This</span> <span class="n">can</span> <span class="n">be</span> <span class="n">either</span> <span class="n">conda</span> <span class="ow">or</span> <span class="n">container</span> <span class="ow">or</span> <span class="n">venv</span><span class="o">.</span> <span class="p">(</span><span class="n">default</span><span class="p">:</span> <span class="n">conda</span><span class="p">)</span>

</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start using template name</span>
<span class="n">llama</span> <span class="n">stack</span> <span class="n">run</span> <span class="n">tgi</span>

<span class="c1"># Start using config file</span>
<span class="n">llama</span> <span class="n">stack</span> <span class="n">run</span> <span class="o">~/.</span><span class="n">llama</span><span class="o">/</span><span class="n">distributions</span><span class="o">/</span><span class="n">llamastack</span><span class="o">-</span><span class="n">my</span><span class="o">-</span><span class="n">local</span><span class="o">-</span><span class="n">stack</span><span class="o">/</span><span class="n">my</span><span class="o">-</span><span class="n">local</span><span class="o">-</span><span class="n">stack</span><span class="o">-</span><span class="n">run</span><span class="o">.</span><span class="n">yaml</span>

<span class="c1"># Start using a venv</span>
<span class="n">llama</span> <span class="n">stack</span> <span class="n">run</span> <span class="o">--</span><span class="n">image</span><span class="o">-</span><span class="nb">type</span> <span class="n">venv</span> <span class="o">~/.</span><span class="n">llama</span><span class="o">/</span><span class="n">distributions</span><span class="o">/</span><span class="n">llamastack</span><span class="o">-</span><span class="n">my</span><span class="o">-</span><span class="n">local</span><span class="o">-</span><span class="n">stack</span><span class="o">/</span><span class="n">my</span><span class="o">-</span><span class="n">local</span><span class="o">-</span><span class="n">stack</span><span class="o">-</span><span class="n">run</span><span class="o">.</span><span class="n">yaml</span>

<span class="c1"># Start using a conda environment</span>
<span class="n">llama</span> <span class="n">stack</span> <span class="n">run</span> <span class="o">--</span><span class="n">image</span><span class="o">-</span><span class="nb">type</span> <span class="n">conda</span> <span class="o">~/.</span><span class="n">llama</span><span class="o">/</span><span class="n">distributions</span><span class="o">/</span><span class="n">llamastack</span><span class="o">-</span><span class="n">my</span><span class="o">-</span><span class="n">local</span><span class="o">-</span><span class="n">stack</span><span class="o">/</span><span class="n">my</span><span class="o">-</span><span class="n">local</span><span class="o">-</span><span class="n">stack</span><span class="o">-</span><span class="n">run</span><span class="o">.</span><span class="n">yaml</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ llama stack run ~/.llama/distributions/llamastack-my-local-stack/my-local-stack-run.yaml

Serving API inspect
 GET /health
 GET /providers/list
 GET /routes/list
Serving API inference
 POST /inference/chat_completion
 POST /inference/completion
 POST /inference/embeddings
...
Serving API agents
 POST /agents/create
 POST /agents/session/create
 POST /agents/turn/create
 POST /agents/delete
 POST /agents/session/delete
 POST /agents/session/get
 POST /agents/step/get
 POST /agents/turn/get

Listening on [&#39;::&#39;, &#39;0.0.0.0&#39;]:8321
INFO:     Started server process [2935911]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://[&#39;::&#39;, &#39;0.0.0.0&#39;]:8321 (Press CTRL+C to quit)
INFO:     2401:db00:35c:2d2b:face:0:c9:0:54678 - &quot;GET /models/list HTTP/1.1&quot; 200 OK
</pre></div>
</div>
</section>
<section id="listing-distributions">
<h2>Listing Distributions<a class="headerlink" href="#listing-distributions" title="Link to this heading"></a></h2>
<p>Using the list command, you can view all existing Llama Stack distributions, including stacks built from templates, from scratch, or using custom configuration files.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="nb">list</span> <span class="o">-</span><span class="n">h</span>
<span class="n">usage</span><span class="p">:</span> <span class="n">llama</span> <span class="n">stack</span> <span class="nb">list</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span>

<span class="nb">list</span> <span class="n">the</span> <span class="n">build</span> <span class="n">stacks</span>

<span class="n">options</span><span class="p">:</span>
  <span class="o">-</span><span class="n">h</span><span class="p">,</span> <span class="o">--</span><span class="n">help</span>  <span class="n">show</span> <span class="n">this</span> <span class="n">help</span> <span class="n">message</span> <span class="ow">and</span> <span class="n">exit</span>
</pre></div>
</div>
<p>Example Usage</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="nb">list</span>
</pre></div>
</div>
</section>
<section id="removing-a-distribution">
<h2>Removing a Distribution<a class="headerlink" href="#removing-a-distribution" title="Link to this heading"></a></h2>
<p>Use the remove command to delete a distribution you’ve previously built.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="n">rm</span> <span class="o">-</span><span class="n">h</span>
<span class="n">usage</span><span class="p">:</span> <span class="n">llama</span> <span class="n">stack</span> <span class="n">rm</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="nb">all</span><span class="p">]</span> <span class="p">[</span><span class="n">name</span><span class="p">]</span>

<span class="n">Remove</span> <span class="n">the</span> <span class="n">build</span> <span class="n">stack</span>

<span class="n">positional</span> <span class="n">arguments</span><span class="p">:</span>
  <span class="n">name</span>        <span class="n">Name</span> <span class="n">of</span> <span class="n">the</span> <span class="n">stack</span> <span class="n">to</span> <span class="n">delete</span> <span class="p">(</span><span class="n">default</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>

<span class="n">options</span><span class="p">:</span>
  <span class="o">-</span><span class="n">h</span><span class="p">,</span> <span class="o">--</span><span class="n">help</span>  <span class="n">show</span> <span class="n">this</span> <span class="n">help</span> <span class="n">message</span> <span class="ow">and</span> <span class="n">exit</span>
  <span class="o">--</span><span class="nb">all</span><span class="p">,</span> <span class="o">-</span><span class="n">a</span>   <span class="n">Delete</span> <span class="nb">all</span> <span class="n">stacks</span> <span class="p">(</span><span class="n">use</span> <span class="k">with</span> <span class="n">caution</span><span class="p">)</span> <span class="p">(</span><span class="n">default</span><span class="p">:</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="n">rm</span> <span class="n">llamastack</span><span class="o">-</span><span class="n">test</span>
</pre></div>
</div>
<p>To keep your environment organized and avoid clutter, consider using <code class="docutils literal notranslate"><span class="pre">llama</span> <span class="pre">stack</span> <span class="pre">list</span></code> to review old or unused distributions and <code class="docutils literal notranslate"><span class="pre">llama</span> <span class="pre">stack</span> <span class="pre">rm</span> <span class="pre">&lt;name&gt;</span></code> to delete them when they’re no longer needed.</p>
</section>
<section id="troubleshooting">
<h2>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Link to this heading"></a></h2>
<p>If you encounter any issues, ask questions in our discord or search through our <a class="reference external" href="https://github.com/meta-llama/llama-stack/issues">GitHub Issues</a>, or file an new issue.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="kubernetes_deployment.html" class="btn btn-neutral float-left" title="Kubernetes Deployment Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../building_applications/index.html" class="btn btn-neutral float-right" title="Building AI Applications (Examples)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Meta.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>