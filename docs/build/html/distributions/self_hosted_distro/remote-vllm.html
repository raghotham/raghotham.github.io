

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Remote vLLM Distribution &mdash; llama-stack  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/my_theme.css?v=f1163765" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />

  
    <link rel="canonical" href="https://github.com/meta-llama/llama-stackdistributions/self_hosted_distro/remote-vllm.html"/>
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=09bf800d"></script>
      <script src="../../_static/js/detect_theme.js?v=76226c80"></script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Meta Reference Distribution" href="meta-reference-gpu.html" />
    <link rel="prev" title="Remote-Hosted Distributions" href="../remote_hosted_distro/index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            llama-stack
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Llama Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/index.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../getting_started/index.html#step-1-install-and-setup">Step 1: Install and setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting_started/index.html#step-2-run-the-llama-stack-server">Step 2: Run the Llama Stack server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting_started/index.html#step-3-run-the-demo">Step 3: Run the demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting_started/index.html#next-steps">Next Steps</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/detailed_tutorial.html">Detailed Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../getting_started/detailed_tutorial.html#step-1-installation-and-setup">Step 1: Installation and Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting_started/detailed_tutorial.html#step-2-run-llama-stack">Step 2:  Run Llama Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting_started/detailed_tutorial.html#step-3-run-client-cli">Step 3: Run Client CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting_started/detailed_tutorial.html#step-4-run-the-demos">Step 4: Run the Demos</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../introduction/index.html">Why Llama Stack?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../introduction/index.html#our-solution-a-universal-stack">Our Solution: A Universal Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../introduction/index.html#our-philosophy">Our Philosophy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../concepts/index.html">Core Concepts</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/index.html#apis">APIs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/index.html#api-providers">API Providers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/index.html#resources">Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/index.html#distributions">Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/index.html#evaluation-concepts">Evaluation Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../concepts/index.html#open-benchmark-eval">Open-benchmark Eval</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../concepts/index.html#list-of-open-benchmarks-llama-stack-support">List of open-benchmarks Llama Stack support</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../concepts/index.html#run-evaluation-on-open-benchmarks-via-cli">Run evaluation on open-benchmarks via CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../concepts/index.html#spin-up-llama-stack-server">Spin up Llama Stack server</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../concepts/index.html#run-eval-cli">Run eval CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../concepts/index.html#whats-next">What’s Next?</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../openai/index.html">OpenAI API Compatibility</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../openai/index.html#server-path">Server path</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../openai/index.html#clients">Clients</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../openai/index.html#llama-stack-client">Llama Stack Client</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../openai/index.html#openai-client">OpenAI Client</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../openai/index.html#apis-implemented">APIs implemented</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../openai/index.html#models">Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../openai/index.html#responses">Responses</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../openai/index.html#simple-inference">Simple inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../openai/index.html#structured-output">Structured Output</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../openai/index.html#chat-completions">Chat Completions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../openai/index.html#id1">Simple inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../openai/index.html#id2">Structured Output</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../openai/index.html#completions">Completions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../openai/index.html#id3">Simple inference</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../providers/index.html">Providers Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../providers/index.html#external-providers">External Providers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../providers/index.html#agents">Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../providers/index.html#datasetio">DatasetIO</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../providers/index.html#eval">Eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../providers/index.html#inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../providers/index.html#post-training">Post Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../providers/index.html#post-training-providers">Post Training Providers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../providers/external.html">External Providers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../providers/post_training/huggingface.html">HuggingFace SFTTrainer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../providers/post_training/torchtune.html">TorchTune</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../providers/post_training/nvidia_nemo.html">NVIDIA NEMO</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../providers/index.html#safety">Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../providers/index.html#scoring">Scoring</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../providers/index.html#telemetry">Telemetry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../providers/index.html#tool-runtime">Tool Runtime</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../providers/index.html#vector-io">Vector IO</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../providers/index.html#vector-io-providers">Vector IO Providers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../providers/external.html">External Providers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../providers/vector_io/faiss.html">Faiss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../providers/vector_io/sqlite-vec.html">SQLite-Vec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../providers/vector_io/chromadb.html">Chroma</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../providers/vector_io/pgvector.html">Postgres PGVector</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../providers/vector_io/qdrant.html">Qdrant</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../providers/vector_io/milvus.html">Milvus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../providers/vector_io/weaviate.html">Weaviate</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Distributions Overview</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../importing_as_library.html">Using Llama Stack as a Library</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../importing_as_library.html#setup-llama-stack-without-a-server">Setup Llama Stack without a Server</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../configuration.html">Configuring a “Stack”</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../configuration.html#providers">Providers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../configuration.html#resources">Resources</a></li>
<li class="toctree-l3"><a class="reference internal" href="../configuration.html#server-configuration">Server Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../configuration.html#authentication-configuration">Authentication Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../configuration.html#quota-configuration">Quota Configuration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../configuration.html#extending-to-handle-safety">Extending to handle Safety</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../list_of_distributions.html">Available List of Distributions</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../list_of_distributions.html#selection-of-a-distribution-template">Selection of a Distribution / Template</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../list_of_distributions.html#distribution-details">Distribution Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../list_of_distributions.html#on-device-distributions">On-Device Distributions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../kubernetes_deployment.html">Kubernetes Deployment Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../kubernetes_deployment.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="../kubernetes_deployment.html#deploying-llama-stack-server-in-kubernetes">Deploying Llama Stack Server in Kubernetes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../kubernetes_deployment.html#verifying-the-deployment">Verifying the Deployment</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../building_distro.html">Build your own Distribution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../building_distro.html#setting-your-log-level">Setting your log level</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_distro.html#llama-stack-build">Llama Stack Build</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_distro.html#running-your-stack-server">Running your Stack server</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_distro.html#listing-distributions">Listing Distributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_distro.html#removing-a-distribution">Removing a Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_distro.html#troubleshooting">Troubleshooting</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../building_applications/index.html">Building AI Applications (Examples)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../building_applications/rag.html">Retrieval Augmented Generation (RAG)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/rag.html#setting-up-vector-dbs">Setting up Vector DBs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/rag.html#ingesting-documents">Ingesting Documents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../building_applications/rag.html#using-precomputed-embeddings">Using Precomputed Embeddings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/rag.html#retrieval">Retrieval</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/rag.html#using-the-rag-tool">Using the RAG Tool</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/rag.html#building-rag-enhanced-agents">Building RAG-Enhanced Agents</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/rag.html#unregistering-vector-dbs">Unregistering Vector DBs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/rag.html#appendix">Appendix</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../building_applications/rag.html#more-ragdocument-examples">More RAGDocument Examples</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../building_applications/agent.html">Agents</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/agent.html#core-concepts">Core Concepts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../building_applications/agent.html#agent-configuration">1. Agent Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../building_applications/agent.html#sessions">2. Sessions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../building_applications/agent.html#turns">3. Turns</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../building_applications/agent.html#non-streaming">Non-Streaming</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../building_applications/agent.html#steps">4. Steps</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/agent.html#agent-execution-loop">Agent Execution Loop</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../building_applications/agent_execution_loop.html">Agent Execution Loop</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/agent_execution_loop.html#steps-in-the-agent-workflow">Steps in the Agent Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/agent_execution_loop.html#agent-execution-loop-example">Agent Execution Loop Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../building_applications/tools.html">Tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/tools.html#server-side-vs-client-side-tool-execution">Server-side vs. client-side tool execution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../building_applications/tools.html#server-side-tools">Server-side tools</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/tools.html#model-context-protocol-mcp">Model Context Protocol (MCP)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../building_applications/tools.html#using-remote-mcp-servers">Using Remote MCP Servers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../building_applications/tools.html#running-your-own-mcp-server">Running your own MCP server</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/tools.html#adding-custom-client-side-tools">Adding Custom (Client-side) Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/tools.html#tool-invocation">Tool Invocation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/tools.html#listing-available-tools">Listing Available Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/tools.html#simple-example-2-using-an-agent-with-the-web-search-tool">Simple Example 2: Using an Agent with the Web Search Tool</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/tools.html#simple-example3-using-an-agent-with-the-wolframalpha-tool">Simple Example3: Using an Agent with the WolframAlpha Tool</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../building_applications/evals.html">Evaluations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/evals.html#application-evaluation">Application Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../building_applications/evals.html#building-a-search-agent">Building a Search Agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../building_applications/evals.html#query-agent-execution-steps">Query Agent Execution Steps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../building_applications/evals.html#evaluate-agent-responses">Evaluate Agent Responses</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../building_applications/telemetry.html">Telemetry</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/telemetry.html#events">Events</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/telemetry.html#spans-and-traces">Spans and Traces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/telemetry.html#sinks">Sinks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/telemetry.html#providers">Providers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../building_applications/telemetry.html#meta-reference-provider">Meta-Reference Provider</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../building_applications/telemetry.html#configuration">Configuration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/telemetry.html#jaeger-to-visualize-traces">Jaeger to visualize traces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../building_applications/telemetry.html#querying-traces-stored-in-sqlite">Querying Traces Stored in SQLite</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../building_applications/safety.html">Safety Guardrails</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../playground/index.html">Llama Stack Playground</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../playground/index.html#key-features">Key Features</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../playground/index.html#playground">Playground</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../playground/index.html#chatbot">Chatbot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../playground/index.html#evaluations">Evaluations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../playground/index.html#inspect">Inspect</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../playground/index.html#starting-the-llama-stack-playground">Starting the Llama Stack Playground</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing/new_api_provider.html">Adding a New API Provider</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../contributing/new_api_provider.html#testing-the-provider">Testing the Provider</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../contributing/new_api_provider.html#integration-testing">1. Integration Testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contributing/new_api_provider.html#unit-testing">2. Unit Testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contributing/new_api_provider.html#additional-end-to-end-testing">3. Additional end-to-end testing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../contributing/new_api_provider.html#submitting-your-pr">Submitting Your PR</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../references/index.html">References</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../references/api_reference/index.html">API Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../references/python_sdk_reference/index.html">Python SDK Reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#shared-types">Shared Types</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#toolgroups">Toolgroups</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#tools">Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#toolruntime">ToolRuntime</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/python_sdk_reference/index.html#ragtool">RagTool</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#agents">Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/python_sdk_reference/index.html#session">Session</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/python_sdk_reference/index.html#steps">Steps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/python_sdk_reference/index.html#turn">Turn</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#batchinference">BatchInference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#datasets">Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#eval">Eval</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/python_sdk_reference/index.html#jobs">Jobs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#inspect">Inspect</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#inference">Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#vectorio">VectorIo</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#vectordbs">VectorDBs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#models">Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#posttraining">PostTraining</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/python_sdk_reference/index.html#job">Job</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#providers">Providers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#routes">Routes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#safety">Safety</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#shields">Shields</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#syntheticdatageneration">SyntheticDataGeneration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#telemetry">Telemetry</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#datasetio">Datasetio</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#scoring">Scoring</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#scoringfunctions">ScoringFunctions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/python_sdk_reference/index.html#benchmarks">Benchmarks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../references/llama_cli_reference/index.html">llama (server-side) CLI Reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../references/llama_cli_reference/index.html#installation">Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/llama_cli_reference/index.html#llama-subcommands"><code class="docutils literal notranslate"><span class="pre">llama</span></code> subcommands</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_cli_reference/index.html#sample-usage">Sample Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../references/llama_cli_reference/index.html#downloading-models">Downloading models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_cli_reference/index.html#downloading-from-meta">Downloading from Meta</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_cli_reference/index.html#downloading-from-hugging-face">Downloading from Hugging Face</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../references/llama_cli_reference/index.html#list-the-downloaded-models">List the downloaded models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/llama_cli_reference/index.html#understand-the-models">Understand the models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_cli_reference/index.html#id1">Sample Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_cli_reference/index.html#describe">Describe</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_cli_reference/index.html#prompt-format">Prompt Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_cli_reference/index.html#remove-model">Remove model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html">llama (client-side) CLI Reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#basic-commands">Basic Commands</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-configure"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">configure</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-providers-list"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">providers</span> <span class="pre">list</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#model-management">Model Management</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-models-list"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">models</span> <span class="pre">list</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-models-get"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">models</span> <span class="pre">get</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-models-register"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">models</span> <span class="pre">register</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-models-update"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">models</span> <span class="pre">update</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-models-delete"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">models</span> <span class="pre">delete</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#vector-db-management">Vector DB Management</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-vector-dbs-list"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">vector_dbs</span> <span class="pre">list</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-vector-dbs-register"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">vector_dbs</span> <span class="pre">register</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-vector-dbs-unregister"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">vector_dbs</span> <span class="pre">unregister</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#shield-management">Shield Management</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-shields-list"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">shields</span> <span class="pre">list</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-shields-register"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">shields</span> <span class="pre">register</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#eval-task-management">Eval Task Management</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-benchmarks-list"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">benchmarks</span> <span class="pre">list</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-benchmarks-register"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">benchmarks</span> <span class="pre">register</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#eval-execution">Eval execution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-eval-run-benchmark"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">eval</span> <span class="pre">run-benchmark</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-eval-run-scoring"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">eval</span> <span class="pre">run-scoring</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#tool-group-management">Tool Group Management</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-toolgroups-list"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">toolgroups</span> <span class="pre">list</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-toolgroups-get"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">toolgroups</span> <span class="pre">get</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-toolgroups-register"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">toolgroups</span> <span class="pre">register</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_stack_client_cli_reference.html#llama-stack-client-toolgroups-unregister"><code class="docutils literal notranslate"><span class="pre">llama-stack-client</span> <span class="pre">toolgroups</span> <span class="pre">unregister</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../references/llama_cli_reference/download_models.html">Downloading Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../references/llama_cli_reference/download_models.html#installation">Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/llama_cli_reference/download_models.html#downloading-models-via-cli">Downloading models via CLI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_cli_reference/download_models.html#downloading-from-meta">Downloading from Meta</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/llama_cli_reference/download_models.html#downloading-from-hugging-face">Downloading from Hugging Face</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../references/llama_cli_reference/download_models.html#list-the-downloaded-models">List the downloaded models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../references/evals_reference/index.html">Evaluations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../references/evals_reference/index.html#evaluation-concepts">Evaluation Concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../references/evals_reference/index.html#evaluation-examples-walkthrough">Evaluation Examples Walkthrough</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/evals_reference/index.html#open-benchmark-model-evaluation">1. Open Benchmark Model Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/evals_reference/index.html#agentic-evaluation">2. Agentic Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/evals_reference/index.html#agentic-application-dataset-scoring">3. Agentic Application Dataset Scoring</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../references/evals_reference/index.html#running-evaluations-via-cli">Running Evaluations via CLI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/evals_reference/index.html#benchmark-evaluation-cli">Benchmark Evaluation CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/evals_reference/index.html#application-evaluation-cli">Application Evaluation CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/evals_reference/index.html#defining-benchmarkconfig">Defining BenchmarkConfig</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../references/evals_reference/index.html#open-benchmark-contributing-guide">Open-benchmark Contributing Guide</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../references/evals_reference/index.html#create-the-new-dataset-for-your-new-benchmark">Create the new dataset for your new benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/evals_reference/index.html#find-scoring-function-for-your-new-benchmark">Find scoring function for your new benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/evals_reference/index.html#add-new-benchmark-into-template">Add new benchmark into template</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../references/evals_reference/index.html#test-the-new-benchmark">Test the new benchmark</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">llama-stack</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Distributions Overview</a></li>
          <li class="breadcrumb-item"><a href="../list_of_distributions.html">Available List of Distributions</a></li>
      <li class="breadcrumb-item active">Remote vLLM Distribution</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/distributions/self_hosted_distro/remote-vllm.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!-- This file was auto-generated by distro_codegen.py, please edit source -->
<section class="tex2jax_ignore mathjax_ignore" id="remote-vllm-distribution">
<h1>Remote vLLM Distribution<a class="headerlink" href="#remote-vllm-distribution" title="Link to this heading"></a></h1>
<div class="toctree-wrapper compound">
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">llamastack/distribution-remote-vllm</span></code> distribution consists of the following provider configurations:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>API</p></th>
<th class="head"><p>Provider(s)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>agents</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>datasetio</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::huggingface</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::localfs</span></code></p></td>
</tr>
<tr class="row-even"><td><p>eval</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>inference</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::vllm</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::sentence-transformers</span></code></p></td>
</tr>
<tr class="row-even"><td><p>safety</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::llama-guard</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>scoring</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::basic</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::llm-as-judge</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::braintrust</span></code></p></td>
</tr>
<tr class="row-even"><td><p>telemetry</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>tool_runtime</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::brave-search</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::tavily-search</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::rag-runtime</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::model-context-protocol</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::wolfram-alpha</span></code></p></td>
</tr>
<tr class="row-even"><td><p>vector_io</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::faiss</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::chromadb</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::pgvector</span></code></p></td>
</tr>
</tbody>
</table>
<p>You can use this distribution if you want to run an independent vLLM server for inference.</p>
<section id="environment-variables">
<h2>Environment Variables<a class="headerlink" href="#environment-variables" title="Link to this heading"></a></h2>
<p>The following environment variables can be configured:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">LLAMA_STACK_PORT</span></code>: Port for the Llama Stack distribution server (default: <code class="docutils literal notranslate"><span class="pre">8321</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">INFERENCE_MODEL</span></code>: Inference model loaded into the vLLM server (default: <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-3.2-3B-Instruct</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">VLLM_URL</span></code>: URL of the vLLM server with the main inference model (default: <code class="docutils literal notranslate"><span class="pre">http://host.docker.internal:5100/v1</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MAX_TOKENS</span></code>: Maximum number of tokens for generation (default: <code class="docutils literal notranslate"><span class="pre">4096</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SAFETY_VLLM_URL</span></code>: URL of the vLLM server with the safety model (default: <code class="docutils literal notranslate"><span class="pre">http://host.docker.internal:5101/v1</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SAFETY_MODEL</span></code>: Name of the safety (Llama-Guard) model to use (default: <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-Guard-3-1B</span></code>)</p></li>
</ul>
</section>
<section id="setting-up-vllm-server">
<h2>Setting up vLLM server<a class="headerlink" href="#setting-up-vllm-server" title="Link to this heading"></a></h2>
<p>In the following sections, we’ll use AMD, NVIDIA or Intel GPUs to serve as hardware accelerators for the vLLM
server, which acts as both the LLM inference provider and the safety provider. Note that vLLM also
<a class="reference external" href="https://docs.vllm.ai/en/latest/getting_started/installation.html">supports many other hardware accelerators</a> and
that we only use GPUs here for demonstration purposes. Note that if you run into issues, you can include the environment variable <code class="docutils literal notranslate"><span class="pre">--env</span> <span class="pre">VLLM_DEBUG_LOG_API_SERVER_RESPONSE=true</span></code> (available in vLLM v0.8.3 and above) in the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> command to enable log response from API server for debugging.</p>
<section id="setting-up-vllm-server-on-amd-gpu">
<h3>Setting up vLLM server on AMD GPU<a class="headerlink" href="#setting-up-vllm-server-on-amd-gpu" title="Link to this heading"></a></h3>
<p>AMD provides two main vLLM container options:</p>
<ul class="simple">
<li><p>rocm/vllm: Production-ready container</p></li>
<li><p>rocm/vllm-dev: Development container with the latest vLLM features</p></li>
</ul>
<p>Please check the <a class="reference external" href="https://rocm.blogs.amd.com/software-tools-optimization/vllm-container/README.html">Blog about ROCm vLLM Usage</a> to get more details.</p>
<p>Here is a sample script to start a ROCm vLLM server locally via Docker:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_PORT</span><span class="o">=</span><span class="m">8000</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span>meta-llama/Llama-3.2-3B-Instruct
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">VLLM_DIMG</span><span class="o">=</span><span class="s2">&quot;rocm/vllm-dev:main&quot;</span>

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--privileged<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16g<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>CAP_SYS_ADMIN<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HUGGING_FACE_HUB_TOKEN=</span><span class="nv">$HF_TOKEN</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HIP_VISIBLE_DEVICES=</span><span class="nv">$CUDA_VISIBLE_DEVICES</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-p<span class="w"> </span><span class="nv">$INFERENCE_PORT</span>:<span class="nv">$INFERENCE_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>~/.cache/huggingface:/root/.cache/huggingface<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">$VLLM_DIMG</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.openai.api_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="nv">$INFERENCE_PORT</span>
</pre></div>
</div>
<p>Note that you’ll also need to set <code class="docutils literal notranslate"><span class="pre">--enable-auto-tool-choice</span></code> and <code class="docutils literal notranslate"><span class="pre">--tool-call-parser</span></code> to <a class="reference external" href="https://docs.vllm.ai/en/latest/features/tool_calling.html">enable tool calling in vLLM</a>.</p>
<p>If you are using Llama Stack Safety / Shield APIs, then you will need to also run another instance of a vLLM with a corresponding safety model like <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-Guard-3-1B</span></code> using a script like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_PORT</span><span class="o">=</span><span class="m">8081</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span>meta-llama/Llama-Guard-3-1B
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">VLLM_DIMG</span><span class="o">=</span><span class="s2">&quot;rocm/vllm-dev:main&quot;</span>

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--privileged<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16g<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>CAP_SYS_ADMIN<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HUGGING_FACE_HUB_TOKEN=</span><span class="nv">$HF_TOKEN</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HIP_VISIBLE_DEVICES=</span><span class="nv">$CUDA_VISIBLE_DEVICES</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-p<span class="w"> </span><span class="nv">$SAFETY_PORT</span>:<span class="nv">$SAFETY_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>~/.cache/huggingface:/root/.cache/huggingface<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">$VLLM_DIMG</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.openai.api_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="nv">$SAFETY_PORT</span>
</pre></div>
</div>
</section>
<section id="setting-up-vllm-server-on-nvidia-gpu">
<h3>Setting up vLLM server on NVIDIA GPU<a class="headerlink" href="#setting-up-vllm-server-on-nvidia-gpu" title="Link to this heading"></a></h3>
<p>Please check the <a class="reference external" href="https://docs.vllm.ai/en/v0.5.5/serving/deploying_with_docker.html">vLLM Documentation</a> to get a vLLM endpoint. Here is a sample script to start a vLLM server locally via Docker:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_PORT</span><span class="o">=</span><span class="m">8000</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span>meta-llama/Llama-3.2-3B-Instruct
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--runtime<span class="w"> </span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpus<span class="w"> </span><span class="nv">$CUDA_VISIBLE_DEVICES</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>~/.cache/huggingface:/root/.cache/huggingface<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HUGGING_FACE_HUB_TOKEN=</span><span class="nv">$HF_TOKEN</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-p<span class="w"> </span><span class="nv">$INFERENCE_PORT</span>:<span class="nv">$INFERENCE_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>vllm/vllm-openai:latest<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpu-memory-utilization<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="nv">$INFERENCE_PORT</span>
</pre></div>
</div>
<p>Note that you’ll also need to set <code class="docutils literal notranslate"><span class="pre">--enable-auto-tool-choice</span></code> and <code class="docutils literal notranslate"><span class="pre">--tool-call-parser</span></code> to <a class="reference external" href="https://docs.vllm.ai/en/latest/features/tool_calling.html">enable tool calling in vLLM</a>.</p>
<p>If you are using Llama Stack Safety / Shield APIs, then you will need to also run another instance of a vLLM with a corresponding safety model like <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-Guard-3-1B</span></code> using a script like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_PORT</span><span class="o">=</span><span class="m">8081</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span>meta-llama/Llama-Guard-3-1B
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">1</span>

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--runtime<span class="w"> </span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpus<span class="w"> </span><span class="nv">$CUDA_VISIBLE_DEVICES</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>~/.cache/huggingface:/root/.cache/huggingface<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HUGGING_FACE_HUB_TOKEN=</span><span class="nv">$HF_TOKEN</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-p<span class="w"> </span><span class="nv">$SAFETY_PORT</span>:<span class="nv">$SAFETY_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>vllm/vllm-openai:latest<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpu-memory-utilization<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="nv">$SAFETY_PORT</span>
</pre></div>
</div>
</section>
<section id="setting-up-vllm-server-on-intel-gpu">
<h3>Setting up vLLM server on Intel GPU<a class="headerlink" href="#setting-up-vllm-server-on-intel-gpu" title="Link to this heading"></a></h3>
<p>Refer to <a class="reference external" href="https://docs.vllm.ai/en/v0.8.2/getting_started/installation/gpu.html?device=xpu">vLLM Documentation for XPU</a> to get a vLLM endpoint. In addition to vLLM side setup which guides towards installing vLLM from sources orself-building vLLM Docker container, Intel provides prebuilt vLLM container to use on systems with Intel GPUs supported by PyTorch XPU backend:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://hub.docker.com/r/intel/vllm">intel/vllm</a></p></li>
</ul>
<p>Here is a sample script to start a vLLM server locally via Docker using Intel provided container:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_PORT</span><span class="o">=</span><span class="m">8000</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span>meta-llama/Llama-3.2-1B-Instruct
<span class="nb">export</span><span class="w"> </span><span class="nv">ZE_AFFINITY_MASK</span><span class="o">=</span><span class="m">0</span>

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>/dev/dri/by-path:/dev/dri/by-path<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>~/.cache/huggingface:/root/.cache/huggingface<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HUGGING_FACE_HUB_TOKEN=</span><span class="nv">$HF_TOKEN</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">ZE_AFFINITY_MASK</span><span class="o">=</span><span class="nv">$ZE_AFFINITY_MASK</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-p<span class="w"> </span><span class="nv">$INFERENCE_PORT</span>:<span class="nv">$INFERENCE_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>intel/vllm:xpu<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpu-memory-utilization<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="nv">$INFERENCE_PORT</span>
</pre></div>
</div>
<p>If you are using Llama Stack Safety / Shield APIs, then you will need to also run another instance of a vLLM with a corresponding safety model like <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-Guard-3-1B</span></code> using a script like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_PORT</span><span class="o">=</span><span class="m">8081</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span>meta-llama/Llama-Guard-3-1B
<span class="nb">export</span><span class="w"> </span><span class="nv">ZE_AFFINITY_MASK</span><span class="o">=</span><span class="m">1</span>

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>/dev/dri/by-path:/dev/dri/by-path<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>~/.cache/huggingface:/root/.cache/huggingface<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HUGGING_FACE_HUB_TOKEN=</span><span class="nv">$HF_TOKEN</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">ZE_AFFINITY_MASK</span><span class="o">=</span><span class="nv">$ZE_AFFINITY_MASK</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-p<span class="w"> </span><span class="nv">$SAFETY_PORT</span>:<span class="nv">$SAFETY_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>intel/vllm:xpu<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpu-memory-utilization<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="nv">$SAFETY_PORT</span>
</pre></div>
</div>
</section>
</section>
<section id="running-llama-stack">
<h2>Running Llama Stack<a class="headerlink" href="#running-llama-stack" title="Link to this heading"></a></h2>
<p>Now you are ready to run Llama Stack with vLLM as the inference provider. You can do this via Conda (build code) or Docker which has a pre-built image.</p>
<section id="via-docker">
<h3>Via Docker<a class="headerlink" href="#via-docker" title="Link to this heading"></a></h3>
<p>This method allows you to get started quickly without having to build the distribution code.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_PORT</span><span class="o">=</span><span class="m">8000</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span>meta-llama/Llama-3.2-3B-Instruct
<span class="nb">export</span><span class="w"> </span><span class="nv">LLAMA_STACK_PORT</span><span class="o">=</span><span class="m">8321</span>

<span class="c1"># You need a local checkout of llama-stack to run this, get it using</span>
<span class="c1"># git clone https://github.com/meta-llama/llama-stack.git</span>
<span class="nb">cd</span><span class="w"> </span>/path/to/llama-stack

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span>:<span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>./llama_stack/templates/remote-vllm/run.yaml:/root/my-run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>llamastack/distribution-remote-vllm<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--config<span class="w"> </span>/root/my-run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">VLLM_URL</span><span class="o">=</span>http://host.docker.internal:<span class="nv">$INFERENCE_PORT</span>/v1
</pre></div>
</div>
<p>If you are using Llama Stack Safety / Shield APIs, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_PORT</span><span class="o">=</span><span class="m">8081</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span>meta-llama/Llama-Guard-3-1B

<span class="c1"># You need a local checkout of llama-stack to run this, get it using</span>
<span class="c1"># git clone https://github.com/meta-llama/llama-stack.git</span>
<span class="nb">cd</span><span class="w"> </span>/path/to/llama-stack

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span>:<span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>~/.llama:/root/.llama<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>./llama_stack/templates/remote-vllm/run-with-safety.yaml:/root/my-run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>llamastack/distribution-remote-vllm<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--config<span class="w"> </span>/root/my-run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">VLLM_URL</span><span class="o">=</span>http://host.docker.internal:<span class="nv">$INFERENCE_PORT</span>/v1<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">SAFETY_VLLM_URL</span><span class="o">=</span>http://host.docker.internal:<span class="nv">$SAFETY_PORT</span>/v1
</pre></div>
</div>
</section>
<section id="via-conda">
<h3>Via Conda<a class="headerlink" href="#via-conda" title="Link to this heading"></a></h3>
<p>Make sure you have done <code class="docutils literal notranslate"><span class="pre">uv</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">llama-stack</span></code> and have the Llama Stack CLI available.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_PORT</span><span class="o">=</span><span class="m">8000</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span>meta-llama/Llama-3.2-3B-Instruct
<span class="nb">export</span><span class="w"> </span><span class="nv">LLAMA_STACK_PORT</span><span class="o">=</span><span class="m">8321</span>

<span class="nb">cd</span><span class="w"> </span>distributions/remote-vllm
llama<span class="w"> </span>stack<span class="w"> </span>build<span class="w"> </span>--template<span class="w"> </span>remote-vllm<span class="w"> </span>--image-type<span class="w"> </span>conda

llama<span class="w"> </span>stack<span class="w"> </span>run<span class="w"> </span>./run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">VLLM_URL</span><span class="o">=</span>http://localhost:<span class="nv">$INFERENCE_PORT</span>/v1
</pre></div>
</div>
<p>If you are using Llama Stack Safety / Shield APIs, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_PORT</span><span class="o">=</span><span class="m">8081</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span>meta-llama/Llama-Guard-3-1B

llama<span class="w"> </span>stack<span class="w"> </span>run<span class="w"> </span>./run-with-safety.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">VLLM_URL</span><span class="o">=</span>http://localhost:<span class="nv">$INFERENCE_PORT</span>/v1<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">SAFETY_VLLM_URL</span><span class="o">=</span>http://localhost:<span class="nv">$SAFETY_PORT</span>/v1
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../remote_hosted_distro/index.html" class="btn btn-neutral float-left" title="Remote-Hosted Distributions" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="meta-reference-gpu.html" class="btn btn-neutral float-right" title="Meta Reference Distribution" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Meta.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>